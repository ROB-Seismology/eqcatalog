# -*- coding: iso-Latin-1 -*-

"""
Processing of online 'Did You Feel It?' (DYFI) data
"""

from __future__ import absolute_import, division, print_function, unicode_literals
from builtins import int

try:
	## Python 2
	basestring
	PY2 = True
except:
	## Python 3
	PY2 = False
	basestring = str


## Third-party modules
import numpy as np

from .dyfi import DYFIEnsemble

__all__ = ["ROBDYFIEnsemble", "ROBMacroseismicEnquiryEnsemble"]


def strip_accents(txt):
	"""
	Remove accents (diacritics) from (unicode) string

	:param txt:
		unicode or str, input string

	:return:
		unicode, output string
	"""
	import unicodedata
	if isinstance(txt, bytes):
		txt = txt.decode("latin1")
	if isinstance(txt, basestring):
		nkfd_form = unicodedata.normalize('NFKD', txt)
		return "".join([c for c in nkfd_form if not unicodedata.combining(c)])
	else:
		return str(txt)


## Disable no-member errors for MacroseismicEnquiryEnsemble
# pylint: disable=no-member


class ROBDYFIEnsemble(DYFIEnsemble):
	"""
	DYFI ensemble linked to ROB database

	:param id_earth:
		int, ID of earthquake in ROB database
	:param recs:
		list of dicts representing enquiries from the database
	"""
	def __init__(self, id_earth, recs):
		self.id_earth = id_earth
		self.recs = recs
		self._gen_arrays()

	def __len__(self):
		return len(self.recs)

	def __repr__(self):
		return '<ROBDYFIEnsemble (n=%d)>' % len(self)

	def __getitem__(self, spec):
		## Note: recs are shared!
		if isinstance(spec, int):
			return self.__class__(self.id_earth, [self.recs[spec]])
		elif isinstance(spec, slice):
			return self.__class__(self.id_earth, self.recs[spec])
		elif isinstance(spec, (list, np.ndarray)):
			## spec can be list of indexes or list of bools
			recs = []
			if len(spec):
				idxs = np.arange(len(self))
				idxs = idxs[np.asarray(spec)]
				for idx in idxs:
					recs.append(self.recs[idx])
			return self.__class__(self.id_earth, recs)

	def _gen_arrays(self):
		"""
		Generate masked arrays from the following record properties:
		- CII
		- CDI
		- MI
		- CWS
		- fiability

		- situation
		- building
		- floor
		- asleep
		- noise
		- felt
		- other_felt
		- motion
		- duration
		- reaction
		- response
		- stand
		- furniture
		- heavy-appliance
		- walls
		- sway
		- creak
		- shelf
		- picture
		- damage

		- id_web

		Note that in many cases, string values are automatically
		converted to floats, and None values to np.nan

		Note: these arrays should not be modified, as they will be
		regenerated by :meth:`set_prop_values` !

		:return:
			None, arrays are stored as class properties
		"""
		for prop in ["CII", "CDI", "MI", "CWS", "fiability"]:
			ar = np.array([rec[prop] for rec in self.recs])
			ar.setflags(write=False)
			setattr(self, prop, ar)

		for prop in ["situation", "building", "floor", "asleep", "noise",
					"felt", "other_felt", "motion", "duration",
					"reaction", "response", "stand", "furniture",
					"heavy_appliance", "walls", "id_web"]:
			prop_list = []
			for rec in self.recs:
				val = rec[prop]
				if val in ('', None) or not (isinstance(val, int) or val.isdigit()):
					val = None
				prop_list.append(val)
			#prop_list = [rec[prop] if rec[prop] != '' else None for rec in self.recs]
			#prop_list = [val if val is not None and (isinstance(val, int) or val.isdigit())
			#			else None for val in prop_list]
			try:
				## Note: dtype='float' ensures that None values are converted to nan
				ar = np.array(prop_list, dtype='float')
			except:
				print("Warning: Array generation failed for prop %s" % prop)
			else:
				#mask = np.isnan(ar)
				#ar = np.ma.array(ar.astype(np.int), mask=mask)
				ar.setflags(write=False)
				setattr(self, prop, ar)

		## Fix other_felt array
		## Only 5 classes are defined, but values in database range from 0 to 5 !
		## other_felt = 1 means no answer, but there are also zero values...
		## The solution is to subtract 1 from values > 0,
		## so that 0 and 1 collapse to the same class
		self.other_felt.setflags(write=True)
		self.other_felt[self.other_felt > 0] -= 1
		## There is even one nan value...
		#self.other_felt[self.other_felt.mask] = 0
		self.other_felt[np.isnan(self.other_felt)] = 0
		self.other_felt.setflags(write=False)

		for prop in ["sway", "creak", "shelf", "picture"]:
			char_map = {c:num for (num, c) in enumerate('ABCDEFGHIJK')}
			char_map['_'] = None
			char_map[''] = None
			prop_list = [rec[prop] or '' for rec in self.recs]
			#mask = np.array([True if val in ('', '_') else False for val in prop_list])
			#ar = np.array(prop_list, dtype='c')
			prop_list = [char_map[val] for val in prop_list]
			ar = np.array(prop_list, dtype='float')
			#ar = np.ma.array(ar, mask=mask)
			ar.setflags(write=False)
			setattr(self, prop, ar)

		self.damage = np.zeros((len(self), 14), dtype='bool')
		for r, rec in enumerate(self.recs):
			damage = rec['d_text']
			damage = [1 if d == '*' else 0 for d in damage]
			self.damage[r] = damage
		self.damage.setflags(write=False)

	def copy(self, deepcopy=False):
		"""
		Return a copy of the ensemble. Note that :prop:`recs` will be
		shared with the original ensemble.

		:param deepcopy:
			bool, if True :prop:`recs` will be copied as well, if False
			they will be shared with the original ensemble
			(default: False)
		:return:
			instance of :class:`MacroseismicEnquiryEnsemble`
		"""
		if not deepcopy:
			return self.__getitem__(slice(None, None, None))
		else:
			recs = [rec.copy() for rec in self.recs]
			return self.__class__(self.id_earth, recs)

	def to_simple_dyfi_ensemble(self, comm_key='id_com', fix_commune_ids=True,
								override_commune_locations=False, fix_felt=True,
								remove_duplicates=False):
		"""
		Convert to simple DYFI ensemble

		Note that, depending on the options specified, the instance
		is modified in place!

		:param comm_key:
			string, commune key to use for setting commune IDs,
			one of "id_com', 'id_main', 'zip'
			(default: 'id_com')
		:param fix_commune_ids:
			bool, whether or not to try fixing missing commune IDs
			(default: True)
		:param override_commune_locations:
			bool, whether or not to override commune locations with
			those from the database. This option should be used when
			not aggregating by commune
			(default: False)
		:param fix_felt:
			bool, whether or not to fix issues where 'felt' property
			is None or zero
			See :meth:`fix_felt_is_none` and :meth:`fix_not_felt`
			(default: True)
		:param remove_duplicates:
			bool, whether or not to automatically remove duplicate
			records
			(default: False)

		:return:
			instance of :class:`DYFIEnsemble`
		"""
		if remove_duplicates:
			ensemble = self.remove_duplicate_records()
		else:
			ensemble = self
		if fix_commune_ids:
			ensemble.fix_commune_ids(keep_existing=True, keep_unmatched=True)
		if comm_key == 'id_main':
			ensemble.set_main_commune_ids()
		if comm_key in ('id_com', 'id_main', 'zip'):
			ensemble.set_locations_from_communes(comm_key=comm_key, max_quality=10,
											keep_unmatched=False)
		if override_commune_locations:
			#if (~self.is_geo_located()).all():
			#	self.set_locations_from_communes(comm_key='id_com', max_quality=5,
			#								keep_unmatched=True)
			ensemble.set_locations_from_geolocation(keep_unmatched=True)
		if fix_felt:
			ensemble.fix_felt_is_none()
			ensemble.fix_not_felt()

		dyfi = super(ROBDYFIEnsemble, ensemble).copy()

		if comm_key == 'id_main':
			dyfi.commune_ids = np.array(self.get_prop_values('id_main'))
		elif comm_key == 'zip':
			dyfi.commune_ids = np.array(self.get_zip_country_tuples())

		return dyfi

	def get_prop_values(self, prop):
		"""
		Get list of values for given property, as read from :prop:`recs`
		Note that type is not converted, and that empty values are
		represented as None values!

		:param prop:
			string, name of property (that can only have certain values)

		:return:
			list
		"""
		if prop == "damage":
			prop = "d_text"
		if not len(self.recs) or not prop in self.recs[0]:
			return []
		else:
			if prop in ["longitude", "latitude", "CII", "CDI", "MI", "CWS",
					"fiability", "situation", "building", "floor", "asleep",
					"noise", "felt", "other_felt", "motion", "duration",
					"reaction", "response", "stand", "furniture",
					"heavy_appliance", "walls"]:
				#none_val = np.nan
				none_val = None
			elif prop in ["d_text", "sway", "creak", "shelf", "picture"]:
				none_val = u""
			else:
				## Note that this may fail if there is only 1 enquiry
				first_non_None_value = next((rec[prop] for rec in self.recs
											if rec[prop] is not None), None)
				if isinstance(first_non_None_value, basestring):
					none_val = u""
				elif first_non_None_value is None:
					none_val = None
				else:
					none_val = np.nan
			return [rec[prop] if rec[prop] is not None else none_val
					for rec in self.recs]

	def set_prop_values(self, prop, values, idxs=None, regenerate_arrays=True):
		"""
		Set values of individual enquiries for given property
		This is done by overwriting the corresponding values in :prop:`recs`,
		optionally followed by regenerating the arrays

		:param prop:
			str, name of property
		:param values:
			list or array, values of individual enquiries for given property
		:param idxs:
			slice, array of indexes or array of bools,
			indexes for which to overwrite values
			(default: None)
		:param regenerate_arrays:
			bool, whether or not to regenerate porperty arrays
			(default: True)
		"""
		if not isinstance(values, (list, tuple, np.ndarray)):
			values = [values] * len(self)
		if idxs is not None:
			ensemble = self.__getitem__(idxs)
		else:
			ensemble = self
		#assert len(values) == len(ensemble)
		for r, rec in enumerate(ensemble.recs):
			rec[prop] = values[r]
		if regenerate_arrays:
			self._gen_arrays()

	def subselect_by_property(self, prop, prop_values, negate=False):
		"""
		Select part of ensemble matching given property values

		:param prop:
			str, name of property
		:param prop_values:
			list of values of :param:`prop` that should be matched
		:param negate:
			bool, whether or not to reverse the matching

		:return:
			instance of :class:`MacroseismicEnquiryEnsemble`
		"""
		if np.isscalar(prop_values):
			prop_values = [prop_values]
		if hasattr(self, prop) and prop != 'id_earth':
			## Regular array
			values = getattr(self, prop)
			idxs = np.zeros_like(values, dtype='bool')
			for pv in prop_values:
				idxs |= np.isclose(values, pv, equal_nan=True)
			if negate:
				idxs = ~idxs
		else:
			## List
			values = self.get_prop_values(prop)
			if len(values) and isinstance(values[0], basestring):
				prop_values = [str(pv) if pv is not None else pv for pv in prop_values]
			if not negate:
				idxs = [i for i in range(len(values)) if values[i] in prop_values]
			else:
				idxs = [i for i in range(len(values)) if not values[i] in prop_values]

		return self.__getitem__(idxs)

	@property
	def longitudes(self):
		return np.array(self.get_prop_values('longitude'), dtype='float')

	@property
	def latitudes(self):
		return np.array(self.get_prop_values('latitude'), dtype='float')

	@property
	def ids(self):
		return np.array(self.get_prop_values('id_web'))

	@property
	def event_ids(self):
		return np.array(self.get_prop_values('id_earth'))

	@property
	def commune_ids(self):
		return np.array(self.get_prop_values('id_com'))

	def get_catalog(self):
		"""
		Fetch earthquake catalog from ROB database

		:return:
			instance of :class:`eqcatalog.EQCatalog`
		"""
		from ..rob.seismodb import query_local_eq_catalog_by_id

		return query_local_eq_catalog_by_id(list(np.unique(self.event_ids)))

	def get_catalog_indexes(self):
		"""
		Determine index for each enquiry in earthquake catalog

		:return:
			int array
		"""
		_, catalog_indexes = np.unique(self.event_ids, return_inverse=True)
		return catalog_indexes

	@property
	def event_longitudes(self):
		idxs = self.get_catalog_indexes()
		catalog = self.get_catalog()
		return catalog.lons[idxs]

	@property
	def event_latitudes(self):
		idxs = self.get_catalog_indexes()
		catalog = self.get_catalog()
		return catalog.lats[idxs]

	@property
	def event_depths(self):
		idxs = self.get_catalog_indexes()
		catalog = self.get_catalog()
		return catalog.get_depths()[idxs]

	@property
	def event_times(self):
		"""
		Event times for each enquiry as reported in database

		:return:
			datetime64 array
		"""
		import datetime
		years = self.get_prop_values('time_year')
		months = self.get_prop_values('time_month')
		days = self.get_prop_values('time_day')
		hrmin = self.get_prop_values('time_hrmin')

		event_times = []
		for i in range(len(self)):
			dt = datetime.datetime(years[i], months[i], days[i]) + hrmin[i]
			event_times.append(dt)
		return np.array(event_times, dtype='datetime64[s]')

	@property
	def submit_times(self):
		"""
		Get submit time of each enquiry
		Note: NULL date values will be replace by the event time
		(see :meth:`get_event_times`)

		:return:
			datetime64 array
		"""
		submit_times = self.get_prop_values('submit_time')
		NULL_DATE = '0000-00-00 00:00:00'
		if NULL_DATE in submit_times:
			event_times = self.event_times
			submit_times = [submit_times[i] if not submit_times[i] == NULL_DATE
							else event_times[i] for i in range(len(self))]

		submit_times = np.array(submit_times, dtype='datetime64[s]')
		return submit_times

	def get_elapsed_times(self, use_enq_event_time=False):
		"""
		Get time interval between earthquake origin time and submit time
		of each enquiry

		:param use_enq_event_time:
			bool, whether or not to use event time reported in enquiry
			If False, earthquake origin time will be fetched from the
			earthquakes database, but this only works if all enquiries
			belong to the same earthquake
			(default: False)

		:return:
			np.timedelta64 array
		"""
		if use_enq_event_time:
			event_times = self.event_times
		else:
			idxs = self.get_catalog_indexes()
			catalog = self.get_catalog()
			event_times = catalog.get_datetimes()[idxs]

		return self.submit_times - event_times

	def subselect_by_zip_country_tuples(self, zip_country_tuples):
		"""
		Select part of ensemble matching given (ZIP, country) tuples

		:param zip_country_tuples:
			list of (ZIP, country) tuples

		:return:
			instance of :class:`MacroseismicEnquiryEnsemble`
		"""
		all_zip_country_tuples = self.get_zip_country_tuples()
		zip_ensemble_recs = []
		for zip_country in zip_country_tuples:
			idxs = [i for i in range(len(self)) if all_zip_country_tuples[i] == zip_country]
			ensemble = self.__getitem__(idxs)
			zip_ensemble_recs.extend(ensemble.recs)
		return self.__class__(self.id_earth, zip_ensemble_recs)

	def get_commune_names(self, main_commune=False):
		"""
		Get names of commune or main commune from ROB database

		:param main_commune:
			bool, whether to fetch commune (False) or main commune (True)

		:return:
			list of str
		"""
		if main_commune:
			comm_key = 'id_main'
			commune_ids = self.get_main_commune_ids()
		else:
			comm_key = 'id_com'
			commune_ids = self.get_prop_values(comm_key)

		commune_recs = self.get_communes_from_db(comm_key)

		return [commune_recs[id_com]['name'] for id_com in commune_ids]

	def subselect_by_commune_name(self, commune_names, main_commune=False):
		"""
		Select DYFI records by name of commune or main commune

		:param commune_names:
			list of str, commune names
		:param main_commune:
			see :meth:`get_commune_names`

		:return:
			instance of :class:`MDPCollection`
		"""
		commune_names = [item.upper() for item in commune_names]
		dyfi_commune_names = self.get_commune_names(main_commune=main_commune)
		idxs = []
		for commune_name in commune_names:
			for m in range(len(self)):
				if dyfi_commune_names[m].upper() in commune_names:
					idxs.append(m)
					break

		return self.__getitem__(idxs)

	def get_addresses(self, exclude_empty_streets=True):
		"""
		Return list of addresses for each record

		:param exclude_empty_streets:
			bool, whether or not to exclude records with empty address field
			(default: True)

		:return:
			list of strings
		"""
		streets = self.get_prop_values('street')
		zips = self.get_prop_values('zip')
		communes = self.get_prop_values('city')
		countries = self.get_prop_values('country')

		addresses = []
		for i in range(len(self)):
			street = streets[i].upper()
			zip = zips[i]
			commune = communes[i].upper()
			country = countries[i].upper()
			zip_commune = "%s %s" % (zip, commune)
			## Sometimes people fill out full address instead of just street
			#street = street.replace(zip_commune, '').strip().rstrip(',')
			street = street.replace('%s' % zip, '')
			street = street.replace(commune, '')
			street = street.strip().rstrip(',').strip()

			if exclude_empty_streets and not street:
				## Filter out empty streets or zips
				address = ""
			else:
				address = "%s, %s, %s"
				address %= (street, zip_commune, country)
			addresses.append(address)
		return addresses

	def geocode(self, provider, bbox=None, start_idx=0, max_requests=100,
				sleep_every=10, **kwargs):
		"""
		Determine locations based on addresses

		:param provider:
			str, name of geocoding provider understood by geocoder
		:param bbox:
			[west, south, east, north] tuple of coordinates
			Not supported by many providers.

		:return:
			list with (lon, lat, confidence) tuples
		"""
		import time
		import geocoder

		results = []
		num_requests = 0
		for address in self.get_addresses():
			success = False
			if address:
				if num_requests < max_requests:
					if num_requests % sleep_every == 0:
						time.sleep(1)
					try:
						g = geocoder.get(address, provider=provider, proximity=bbox,
										**kwargs)
					except:
						pass
					else:
						num_requests += 1
						if g.ok:
							success = True

			if success:
				if (bbox and (bbox[0] <= g.lng <= bbox[2])
						and (bbox[1] <= g.lat <= bbox[3])):
					results.append((g.lng, g.lat, g.confidence))
				else:
					success = False
			if not success:
				results.append(tuple())

		return results

	def get_zip_country_tuples(self):
		"""
		Return list of (ZIP, country) tuples for each record

		:return:
			list of (ZIP, country) tuples
		"""
		zips = map(int, self.get_prop_values('zip'))
		countries = self.get_prop_values('country')
		zip_country_tuples = list(zip(zips, countries))
		return zip_country_tuples

	def get_unique_zip_country_tuples(self):
		"""
		Return list of unique (ZIP, country) tuples in ensemble

		:return:
			list of (ZIP, country) tuples
		"""
		import operator

		zip_country_tuples = set(self.get_zip_country_tuples())
		return sorted(zip_country_tuples, key=operator.itemgetter(1, 0))

	def get_communes_from_db(self, comm_key='id_com', verbose=False):
		"""
		Extract communes from database

		:param comm_key:
			string, commune key, one of "id_com', 'id_main', 'zip'
			(default: 'id_com')
		:param verbose:
			bool, whether or not to print SQL queries
			(default: False)

		:return:
			dict, mapping comm_key values to database records (dicts)
		"""
		from ..rob.seismodb import query_seismodb_table, get_communes
		from difflib import SequenceMatcher as SM

		if comm_key in ("id_com", "id_main"):
			if comm_key == "id_main" and not hasattr(self, "id_main"):
				self.set_main_commune_ids()
			unique_ids = sorted(set(self.get_prop_values(comm_key)))
			comm_recs = get_communes(country='', id_com=unique_ids, verbose=verbose)
			#table_clause = ['communes']
			#column_clause = ['*']
			#query_values = ','.join(map(str, unique_ids))
			#where_clause = 'id in (%s)' % query_values
			#comm_recs = query_seismodb_table(table_clause, column_clause,
			#					where_clause=where_clause, verbose=verbose)
			comm_rec_dict = {rec['id']: rec for rec in comm_recs}
		elif comm_key == "zip":
			comm_rec_dict = {}
			column_clause = ['*']
			for country in ("BE", "NL", "DE", "FR", "LU", "GB"):
				## Some tables have commune, some city, some both...
				## BE, LU, NL: city < commune
				## DE, FR: commune
				## GB: city
				if country in ("BE", "LU", "NL", "GB"):
					com_col = 'city'
				else:
					com_col = 'commune'

				if country == "BE":
					com_tables = ['com_zip_BE_fr', 'com_zip_BE_nl']
				elif country == "LU":
					com_tables = ['com_zip_LU_de', 'com_zip_LU_fr']
				else:
					com_tables = ['com_zip_%s' % country]
				#table_clause = 'com_zip_%s' % country
				#if country in ("BE", "LU"):
				#	table_clause += '_fr'

				ensemble = self.subselect_by_property('country', [country])
				zips = ensemble.get_prop_values('zip')
				unique_zips = sorted(set(zips))
				if len(unique_zips):
					cities = [strip_accents(city).title()
							for city in ensemble.get_prop_values('city')]
					unique_zip_cities = set(zip(zips, cities))
					#join_clause = [('RIGHT JOIN', 'communes', '%s.id = communes.id_main' % table_clause)]

					country_comm_rec_dict = {}
					if country == "NL":
						query_values = '|'.join(['%s' % ZIP for ZIP in unique_zips if ZIP])
						where_clause = 'zip REGEXP "%s"' % query_values
					else:
						query_values = ','.join(['"%s"' % ZIP for ZIP in unique_zips if ZIP])
						where_clause = 'zip IN (%s)' % query_values
					for table in com_tables:
						table_clause = table
						comm_recs = query_seismodb_table(table_clause, column_clause,
										where_clause=where_clause, verbose=verbose)
						if country == "NL":
							comm_recs = {(country, rec['zip'][:-3],
									strip_accents(rec[com_col]).title()): rec
									for rec in comm_recs}
						else:
							comm_recs = {(country, rec['zip'],
									strip_accents(rec[com_col]).title()): rec
									for rec in comm_recs}
						country_comm_rec_dict.update(comm_recs)

					for (ZIP, city) in sorted(unique_zip_cities):
						#try:
						key = (country, ZIP, strip_accents(city).title())
						#except:
						#	key = (country, ZIP, city)
						rec = country_comm_rec_dict.get(key)
						if rec:
							## Zip and commune name matched
							comm_rec_dict[key] = rec
						else:
							## Only zip matched, try fuzzy text matching
							country, ZIP, city = key
							matching_zips, match_ratios, id_coms = [], [], []
							for k in country_comm_rec_dict.keys():
								if k[1] == ZIP:
									#matching_zips.append(country_comm_rec_dict[k])
									matching_zips.append(k)
									match_ratios.append(SM(None, k[2], city).ratio())
									id_coms.append(country_comm_rec_dict[k]['id_com'])
							#id_coms = [r['id_com'] for r in matching_zips]
							if len(matching_zips):
								idx = np.argmax(match_ratios)
								if match_ratios[idx] >= 0.4:
									comm_rec_dict[key] = country_comm_rec_dict[matching_zips[idx]]
									if verbose:
										msg = "Commune %s-%s: %s was fuzzy-matched with %s"
										msg %= (country, ZIP, city, matching_zips[idx][2])
										print(msg)
								else:
									## Take smallest id_com
									idx = np.argmin(id_coms)
									#comm_rec_dict[key] = matching_zips[idx]
									comm_rec_dict[key] = country_comm_rec_dict[matching_zips[idx]]
									if verbose:
										msg = "Commune %s-%s: %s was matched with main commune %s"
										msg %= (country, ZIP, city, matching_zips[idx][2])
										print(msg)
							elif verbose:
								## Unmatched ZIP, probably wrong
								# TODO: we could still try to match commune name?
								msg = "Commune %s-%s: %s could not be matched"
								msg %= (country, ZIP, city)
								print(msg)
					"""
					for rec in comm_recs:
						if country == "NL":
							ZIP = rec['zip'][:-3]
						else:
							ZIP = rec['zip']
						key = (ZIP, country)
						if not key in comm_rec_dict:
							comm_rec_dict[key] = rec
						else:
							## There may be more than one record with the same ZIP...
							## The lowest id_com should correspond to id_main
							# TODO: try matching name first (city/commune, city,
							# we will also need lang...)
							if rec['id_com'] < comm_rec_dict[key]['id_com']:
								comm_rec_dict[key] = rec
					"""
		return comm_rec_dict

	def fix_commune_ids(self, keep_existing=True, keep_unmatched=False,
						verbose=True):
		"""
		Reset commune ID of all records based on ZIP and country

		:param keep_existing:
			bool, whether or not to keep existing (= non-zero) commune ids
			(default: True)
		:param keep_unmatched:
			bool, whether or not to keep current commune id of unmatched
			records
			(default: False)
		:param verbose:
			bool, whether or not to print information about operation
			(default: True)

		:return:
			None, 'id_com' values of :prop:`recs` are modified in place
		"""
		comm_rec_dict = self.get_communes_from_db(comm_key='zip')
		num_zip_name_fixed, num_zip_fixed, num_unmatched = 0, 0, 0
		for rec in self.recs:
			if not (rec['id_com'] and keep_existing):
				#comm_rec = comm_rec_dict.get((rec['zip'], rec['country']))
				comm_name = rec['city']
				if comm_name:
					comm_name = comm_name.title()
				comm_rec = comm_rec_dict.get((rec['country'], rec['zip'], comm_name))
				if comm_rec:
					## Zip and name matched
					rec['id_com'] = comm_rec['id_com']
					num_zip_name_fixed += 1
				else:
					comm_rec = comm_rec_dict.get((rec['country'], rec['zip'], u''))
					if comm_rec:
						## Only zip matched
						rec['id_com'] = comm_rec['id_com']
						num_zip_fixed += 1
					elif not keep_unmatched:
						## Nothing matched
						rec['id_com'] = 0
						num_unmatched += 1

		if verbose:
			print('Fixed %d communes by zip/name, %d by zip only, %d unmatched'
					% (num_zip_name_fixed, num_zip_fixed, num_unmatched))

	def get_main_commune_ids(self):
		"""
		Get IDs of main communes for each record

		:return:
			list of ints, main commune IDs
		"""
		comm_rec_dict = self.get_communes_from_db(comm_key='id_com')
		main_commune_ids = []
		for rec in self.recs:
			comm_rec = comm_rec_dict.get(rec['id_com'])
			if comm_rec:
				main_commune_ids.append(comm_rec['id_main'])
			else:
				main_commune_ids.append(None)
		return main_commune_ids

	def set_main_commune_ids(self, keep_existing=True):
		"""
		Set main commune ID of all records based on id_com

		:return:
			None, 'id_main' values of :prop:`recs` are created or
			modified in place
		"""
		main_commune_ids = self.get_main_commune_ids()
		for r, rec in enumerate(self.recs):
			## Note: records do not initially have 'id_main' key!
			if not (rec.get('id_main') and keep_existing):
				rec['id_main'] = main_commune_ids[r]

	def set_locations_from_communes(self, comm_key="id_com", keep_unmatched=True,
									max_quality=5, verbose=True):
		"""
		Set location of all records from corresponding communes

		:param comm_key:
			see :meth:`get_communes_from_db`
		:param keep_unmatched:
			bool, keep unmatched records untouched (True) or set to nan
			(False)
		:param max_quality:
			int, maximum location quality, only overwrite location
			if location quality is <= max_quality
			(default: 5)
		:param verbose:
			bool, whether or not to print information about operation
			(default: True)

		:return:
			None, 'longitude' and 'latitude' values of :prop:`recs`
			are created or modified in place
		"""
		comm_rec_dict = self.get_communes_from_db(comm_key=comm_key)
		location_quality_dict = {'id_com': 5, 'zip': 5, 'id_main': 4}
		num_located, num_zeroed = 0, 0
		for rec in self.recs:
			if not rec['location_quality'] or rec['location_quality'] <= max_quality:
				if comm_key in ("id_com", "id_main"):
					key = rec[comm_key]
				elif comm_key == "zip":
					key = (rec[comm_key], rec['country'])
				comm_rec = comm_rec_dict.get(key)
				if comm_rec:
					rec['longitude'] = comm_rec['longitude']
					rec['latitude'] = comm_rec['latitude']
					# TODO: decide on location quality
					rec['location_quality'] = location_quality_dict[comm_key]
					num_located += 1
				elif not keep_unmatched:
					rec['longitude'] = rec['latitude'] = rec['location_quality'] = np.nan
					num_zeroed += 1

		if verbose:
			print('Set location for %d records, removed location for %d unmatched records'
					% (num_located, num_zeroed))

	def read_locations_from_db(self):
		"""
		Read locations for each record from the database

		:return:
			dict, mapping id_web to database records (with 'longitude',
			'latitude' and 'quality' keys)
		"""
		from ..rob.seismodb import query_seismodb_table

		table_clause = ['web_location']
		column_clause = ['*']
		web_ids = self.get_prop_values('id_web')
		query_values = ','.join(map(str, web_ids))
		where_clause = 'id_web in (%s)' % query_values
		db_recs = query_seismodb_table(table_clause, column_clause,
										where_clause=where_clause)
		db_rec_dict = {rec['id_web']: rec for rec in db_recs}
		return db_rec_dict

	def set_locations_from_geolocation(self, keep_unmatched=True, verbose=True):
		"""
		Set location of all records from geolocation in database

		:param keep_unmatched:
			bool, keep unmatched records untouched (True) or set to nan
			(False)
			(default: True)
		:param verbose:
			bool, whether or not to print information about operation
			(default: True)

		:return:
			None, 'longitude' and 'latitude' values of :prop:`recs`
			are created or modified in place
		"""
		db_rec_dict = self.read_locations_from_db()
		num_located, num_zeroed = 0, 0
		for rec in self.recs:
			db_rec = db_rec_dict.get(rec['id_web'])
			if db_rec:
				rec['longitude'] = db_rec['longitude']
				rec['latitude'] = db_rec['latitude']
				rec['location_quality'] = db_rec['quality']
				num_located += 1
			elif not keep_unmatched:
				rec['longitude'] = rec['latitude'] = rec['location_quality'] = np.nan
				num_zeroed += 1

		if verbose:
			print('Set location for %d records, removed location for %d unmatched records'
					% (num_located, num_zeroed))

	def write_locations_to_db(self, user, passwd, min_quality=6, overwrite=False,
							dry_run=False):
		"""
		Write locations to database

		:param user:
			str, name of user with write permission
		:param passwd:
			str, password for given user
		:param min_quality:
			int, minimum location quality to write to database
			(default: 6)
		:param overwrite:
			bool, whether or not existing locations should be overwritten
			(default: False)
		:param dry_run:
			bool, whether to actually write locations to database (False)
			or just report how many records would be added/modified (True)
			(default: False)
		"""
		import db.simpledb as simpledb
		from secrets.seismodb import host, database

		db_rec_dict = self.read_locations_from_db()
		recs_to_add, recs_to_modify = [], []
		for rec in self.recs:
			if rec['location_quality'] >= min_quality:
				if rec['id_web'] in db_rec_dict:
					if overwrite:
						recs_to_modify.append(rec)
				else:
					recs_to_add.append(rec)

		## Write to database
		seismodb = simpledb.MySQLDB(database, host, user, passwd)
		table_name = 'web_location'
		if len(recs_to_add):
			print("Adding %d new records" % len(recs_to_add))
			if not dry_run:
				seismodb.add_records(table_name, recs_to_add)
		if len(recs_to_modify):
			print("Updating %d existing records" % len(recs_to_modify))
			if not dry_run:
				seismodb.update_rows(table_name, recs_to_modify, 'id_web')

	def get_bad_zip_country_tuples(self):
		"""
		Get list of (ZIP, country) tuples that cannot be matched
		with database

		Note that this is not necessarily problematic, as the 'communes'
		table seems to be missing some ZIP values that are present
		in com_zip_BE_nl...

		:return:
			list with (ZIP, country) tuples
		"""
		import operator

		zip_country_tuples = self.get_unique_zip_country_tuples()
		zip_country_tuples = set([(str(zip), country)
									for zip, country in zip_country_tuples])
		comm_recs = self.get_communes_from_db().values()
		db_zip_country_tuples = set([(rec['code_p'], rec['country']) for rec in comm_recs])
		bad_zip_country_tuples = zip_country_tuples.difference(db_zip_country_tuples)
		bad_zip_country_tuples = sorted(bad_zip_country_tuples, key=operator.itemgetter(1, 0))
		return [(int(zip), country) for zip, country in bad_zip_country_tuples]

	def get_bad_zip_ensemble(self):
		bad_zip_country_tuples = self.get_bad_zip_country_tuples()
		return self.subselect_by_zip_country_tuples(bad_zip_country_tuples)

	def split_by_commune(self, comm_key='id_com'):
		"""
		Split enquiries by commune

		:param comm_key:
			see :meth:`get_communes_from_db`

		:return:
			dict, mapping comm_key values to instances of
			:class:`MacroseismicEnquiryEnsemble`
		"""
		comm_rec_dict = self.get_communes_from_db(comm_key=comm_key)
		if comm_key in ('id_com', 'id_main'):
			all_comm_key_values = self.get_prop_values(comm_key)
		elif comm_key == "zip":
			all_comm_key_values = self.get_zip_country_tuples()
		comm_ensemble_dict = {}
		for comm_key_val in comm_rec_dict.keys():
			idxs = [i for i in range(len(self)) if all_comm_key_values[i] == comm_key_val]
			ensemble = self.__getitem__(idxs)
			comm_ensemble_dict[comm_key_val] = ensemble
		return comm_ensemble_dict

	def aggregate(self, aggregate_by='id_com', min_replies=3,
					min_fiability=80, filter_floors=(0, 4),
					agg_info='cii', agg_method='mean',
					include_other_felt=True, include_heavy_appliance=False,
					max_deviation=2., max_nan_pct=100,
					fix_commune_ids=True, fix_felt=True, remove_duplicates=False,
					**kwargs):
		"""
		Get aggregated macroseismic information.

		:param aggregate_by:
		:param min_replies:
		:param min_fiability:
		:param filter_floors:
		:param agg_info:
		:param agg_method:
		:param include_other_felt:
		:param include_heavy_appliance:
		:param max_deviation:
		:param max_nan_pct:
		:**kwargs:
			see :meth:`DYFIEnsemble.aggregate`
		:param fix_commune_ids:
		:param fix_felt:
		:param remove_duplicates:
			see :meth:`to_simple_dyfi_ensemble`

		:return:
			instance of :class:`AggregatedMacroInfoCollection`
		"""
		if aggregate_by == 'commune':
			aggregate_by = 'id_com'
		elif aggregate_by == 'main commune':
			aggregate_by = 'id_main'

		if aggregate_by in ('id_com', 'id_main', 'zip'):
			comm_key = aggregate_by
			override_commune_locations = False
		else:
			comm_key = 'id_com'
			override_commune_locations = True

		dyfi = self.to_simple_dyfi_ensemble(comm_key, fix_felt=fix_felt,
											fix_commune_ids=fix_commune_ids,
											remove_duplicates=remove_duplicates)

		return dyfi.aggregate(aggregate_by, min_replies=min_replies,
					min_fiability=min_fiability, filter_floors=filter_floors,
					agg_info=agg_info, agg_method=agg_method,
					include_other_felt=include_other_felt,
					include_heavy_appliance=include_heavy_appliance,
					max_deviation=max_deviation, max_nan_pct=max_nan_pct,
					**kwargs)

	def aggregate_by_commune(self, comm_key='id_main',
					min_replies=3, min_fiability=80, filter_floors=(0, 4),
					agg_info='cii', agg_method='mean',
					include_other_felt=True, include_heavy_appliance=False,
					max_deviation=2., max_nan_pct=100,
					fix_commune_ids=True, fix_felt=True):
		"""
		Aggregate DYFI collection by commune

		:param comm_key:
			see :meth:`get_communes_from_db`

		See :meth:`aggregate` for other parameters

		:return:
			instance of :class:`AggregatedMacroInfoCollection`
		"""
		aggregate_by = comm_key
		agg_macro_coll = self.aggregate(aggregate_by, min_replies=min_replies,
						min_fiability=min_fiability, filter_floors=filter_floors,
						agg_info=agg_info, agg_method=agg_method,
						include_other_felt=include_other_felt,
						include_heavy_appliance=include_heavy_appliance,
						max_deviation=max_deviation, max_nan_pct=max_nan_pct,
						fix_commune_ids=fix_commune_ids, fix_felt=fix_felt)

		## Override agg_type, as DYFIEnsemble only knows id_com
		if comm_key in ('id_main', 'zip'):
			agg_macro_coll.agg_type = 'id_main'
			for agg_macro in agg_macro_coll:
				agg_macro.agg_type = 'id_main'

		return agg_macro_coll

	def split_by_zip(self):
		"""
		Split DYFI ensemble based on ZIP (and country)

		:return:
			dict, mapping (ZIP, country) tuples to instances of
			:class:`ROBDYFIEnsemble`
		"""
		all_zip_country_tuples = self.get_zip_country_tuples()
		unique_zip_country_tuples = set(all_zip_country_tuples)
		zip_ensemble_dict = {}
		for zip_country in unique_zip_country_tuples:
			idxs = [i for i in range(len(self)) if all_zip_country_tuples[i] == zip_country]
			ensemble = self.__getitem__(idxs)
			zip_ensemble_dict[zip_country] = ensemble
		return zip_ensemble_dict

	def fix_all(self, verbose=False):
		"""
		Fix various issues:
		- repair records that have 'felt' unspecified
		- set 'motion', 'reaction' and 'stand' to 0 for not-felt records
		- match unmatched commune IDs
		- set main commune IDs
		- remove duplicate records
		- recompute fiabilities

		:param verbose:
			bool, whether or not to print information about
			duplicate records
			(default: False)

		:return:
			instance of :class:`MacroseismicEnquiryEnsemble`
		"""
		ensemble = self.copy()
		ensemble.fix_felt_is_none(verbose=verbose)
		ensemble.fix_not_felt(verbose=verbose)
		ensemble.fix_commune_ids(verbose=verbose)
		if len(ensemble) > 0:
			ensemble.set_main_commune_ids()
		ensemble = ensemble.remove_duplicate_records(verbose=verbose)
		fiabilities = ensemble.calc_fiability(include_other_felt=False,
											include_heavy_appliance=True)
		ensemble.set_prop_values('fiability', fiabilities)
		return ensemble

	def calc_fiability(self, include_other_felt=True, include_heavy_appliance=False,
						aggregate=False, filter_floors=False):
		"""
		Compute reliability of individual enquiries following ROB web
		procedure

		:param include_other_felt:
		:param include_heavy_appliance:
			see :meth:`calc_cws`
		:param aggregate:
		:param filter_floors:
			dummy arguments to have same call signature as :meth:`calc_cws`
			will be ignored

		:return:
			float array, fiabilities
		"""
		emails = self.get_prop_values('email')
		felt_indexes = self.calc_felt_index(include_other_felt)
		motion_indexes = self.calc_motion_index().filled(0)
		reaction_indexes = self.calc_reaction_index().filled(0)
		stand_indexes = self.calc_stand_index().filled(0)
		shelf_indexes = self.calc_shelf_index().filled(0)
		picture_indexes = self.calc_picture_index().filled(0)
		furniture_indexes = self.calc_furniture_index(include_heavy_appliance).filled(0)
		damage_indexes = self.calc_damage_index()

		fiability = np.zeros(len(self))
		for i in range(len(self)):
			fiability[i] = 80

			if emails[i]:
				fiability[i] += 10

			if felt_indexes[i] == 0:
				if motion_indexes[i] > 0:
					fiability[i] -= (10 * motion_indexes[i])
				if reaction_indexes[i] > 0:
					fiability[i] -= (10 * reaction_indexes[i])
				if stand_indexes[i] > 0:
					fiability[i] -= 50
				if (shelf_indexes[i] > 1 or furniture_indexes[i] > 0
					or damage_indexes[i] > 1.5):
					fiability[i] -= (20 * damage_indexes[i])
			else:
				if (stand_indexes[i] == 1 and
					(motion_indexes[i] < 3 or reaction_indexes[i] < 2)):
					fiability[i] -= 30
				elif (motion_indexes[i] < 3 and reaction_indexes[i] > 3):
					fiability[i] -= 30

			if (damage_indexes[i] > 2 and shelf_indexes[i] < 2
				and picture_indexes[i] == 0):
				fiability[i] -= ((damage_indexes[i] - shelf_indexes[i]) * 20)

		fiability = np.maximum(0, np.minimum(100, fiability))
		return fiability

	def report_by_commune(self, comm_key='id_com', sort_column=0, sort_order="asc"):
		"""
		Print a sorted table of commune names, number of replies,
		mean CCI in database and aggregated CII

		:param comm_key:
			str, commune key, either 'id_com' or 'id_main'
			(default: 'id_com')
		:param sort_column:
			int, column number to sort table with
			(default: 0)
		:param sort_order:
			str, either "asc" (ascending) or "desc" (descending)
			(default: "asc")
		"""
		from operator import itemgetter
		from prettytable import PrettyTable

		table = PrettyTable(["Commune", "ID", "Num replies", "Mean CII", "Aggregated CII"])
		comm_ensemble_dict = self.split_by_commune(comm_key)
		comm_rec_dict = self.get_communes_from_db(comm_key)
		for comm_id, ensemble in comm_ensemble_dict.items():
			comm_name = comm_rec_dict[comm_id]['name']
			mean_cii = np.mean(ensemble.CII)
			agg_cii = ensemble.calc_cii(filter_floors=(0,4), include_other_felt=True)
			table.add_row([comm_name, comm_id, len(ensemble), "%.1f" % mean_cii,
							"%.1f" % agg_cii])

		reverse_order = {"asc": False, "desc": True}[sort_order]
		table._rows = sorted(table._rows, key=itemgetter(sort_column), reverse=reverse_order)
		print(table)

	def find_duplicate_addresses(self, allow_multiple_names=True, verbose=True):
		"""
		Find duplicate records based on their address (street name and ZIP).

		:param allow_multiple_names:
			bool, whether or not to allow multiple names (= persons) at the
			same address
			(default: True)
		:param verbose:
			bool, whether or not to print some useful information

		:return:
			list of lists containing indexes of duplicate records
		"""
		all_streets = self.get_prop_values('street')
		all_zips = self.get_prop_values('zip')
		#all_communes = self.get_prop_values('city')
		all_names = self.get_prop_values('name')
		unique_streets = []
		unique_idxs = []
		duplicate_idxs = {}
		for s, street in enumerate(all_streets):
			## Only consider non-empty strings containing numbers
			#if street and not street.replace(' ', '').isalpha():
			if street and any(char.isdigit() for char in street.replace(' ', '')):
				## Combine with ZIP
				zip_street = (all_zips[s], street)
				if not zip_street in unique_streets:
					unique_streets.append(zip_street)
					unique_idxs.append(s)
				else:
					## Duplicate
					unique_idx = unique_streets.index(zip_street)
					unique_idx = unique_idxs[unique_idx]
					if not allow_multiple_names or all_names[s] == all_names[unique_idx]:
						if unique_idx in duplicate_idxs:
							duplicate_idxs[unique_idx].append(s)
						else:
							duplicate_idxs[unique_idx] = [s]
		duplicate_idxs = [[key] + values for key, values in duplicate_idxs.items()]
		duplicate_idxs = sorted(duplicate_idxs)

		if verbose:
			print("Duplicate streets:")
			for idxs in duplicate_idxs:
				ensemble = self.__getitem__(idxs)
				for rec in ensemble.recs:
					street = rec['street']
					if street:
						street = street.encode('ascii', 'replace')
					zip = rec['zip']
					cii = rec['CII']
					fiability = rec['fiability']
					name = rec['name']
					if name:
						name = name.encode('ascii', errors='replace')
					print("  %s [CII=%s, fiab=%d] %s - %s" % (zip, cii, fiability,
						name, street))
				print("")

		return duplicate_idxs

	def get_duplicate_records(self, allow_multiple_names=True, verbose=True):
		"""
		Get duplicate records based on their address (street name and ZIP).

		:param allow_multiple_names:
		:param verbose:
			see :meth:`find_duplicate_addresses`

		:return:
			list of instances of :class:`MacroseismicEnquiryEnsemble`
			for each set of duplicate records
		"""
		duplicate_idxs = self.find_duplicate_addresses(verbose=verbose,
											allow_multiple_names=allow_multiple_names)
		ensemble_list = []
		for idxs in duplicate_idxs:
			ensemble = self.__getitem__(idxs)
			ensemble_list.append(ensemble)
		return ensemble_list

	def remove_duplicate_records(self, allow_multiple_names=True, verbose=True):
		"""
		Remove duplicate records based on street name and ZIP code.
		For duplicate records, the one with the highest fiability and
		most recent submit time is kept.

		Note that records are not removed from current instance,
		but a new instance without the duplicate records is returned!

		:param allow_multiple_names:
		:param verbose:
			see :meth:`find_duplicate_addresses`

		:return:
			instance of :class:`MacroseismicEnquiryEnsemble`
		"""
		duplicate_idxs = self.find_duplicate_addresses(verbose=verbose,
											allow_multiple_names=allow_multiple_names)
		web_ids_to_remove = []
		for idxs in duplicate_idxs:
			ensemble = self.__getitem__(idxs)
			## Keep the one with highest fiability and most recent submit time
			# TODO: should we also consider CII (lowest = most reliable?)
			submit_times = ensemble.get_prop_values('submit_time')
			ft = list(zip(ensemble.fiability, submit_times))
			## numpy argsort doesn't work for tuples, this works similar
			#order = np.argsort(fiability_times)
			order = sorted(range(len(ft)), key=ft.__getitem__)
			subensemble = ensemble[order[:-1]]
			web_ids_to_remove.extend(subensemble.get_prop_values('id_web'))

		if verbose:
			print("Removing %d duplicates" % len(web_ids_to_remove))
		return self.subselect_by_property('id_web', web_ids_to_remove, negate=True)


ROBMacroseismicEnquiryEnsemble = ROBDYFIEnsemble
